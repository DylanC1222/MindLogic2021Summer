{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "lucy lucas model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVXrNOO5AprH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af200c5c-ce40-495c-d4fe-d400583c84cb"
      },
      "source": [
        "!pip install transformers --quiet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.6 MB 15.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 53.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 636 kB 59.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 58.5 MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsXbK-VMAwjK"
      },
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import transformers\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import DistilBertModel, DistilBertTokenizer, ElectraModel, ElectraTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9_Fm3EJAyqB"
      },
      "source": [
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCcfp91wA2x_"
      },
      "source": [
        "MAX_LEN = 50\n",
        "TRAIN_BATCH_SIZE = 16\n",
        "VALID_BATCH_SIZE = 16\n",
        "EPOCHS = 20\n",
        "LEARNING_RATE = 5e-05\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "etokenizer = ElectraTokenizer.from_pretrained('google/electra-small-discriminator')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Elgrh-VYBBkw"
      },
      "source": [
        "class Triage(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.len = len(dataframe)\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        title = str(self.data.message[index])\n",
        "        title = \" \".join(title.split())\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            title,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'pn_targets': torch.tensor(self.data.encode_pn[index], dtype=torch.long),\n",
        "            'tense_targets': torch.tensor(self.data.encode_tense[index], dtype=torch.long),\n",
        "            'form_targets': torch.tensor(self.data.encode_form[index], dtype=torch.long),\n",
        "            'subject_targets': torch.tensor(self.data.encode_subject[index], dtype=torch.long),\n",
        "            'primary_targets': torch.tensor(self.data.encode_primary[index], dtype=torch.long)\n",
        "        } \n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvemyA_qJCEv"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpFFxynbBWKv"
      },
      "source": [
        "# Reading in data from CSV with \n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Lucy Lucas Classification Model/encoded_model_data.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADWdLsu6BZNp"
      },
      "source": [
        "df = df[['message', 'positive', 'tense', 'form', 'subject', 'primary', 'secondary', 'tertiary', 'classification', 'intent']]\n",
        "df = df.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMhNmM4DozoV"
      },
      "source": [
        "# Creating a subset of unique intent values so there are no missing training sentences in the validation set\n",
        "hand_df = df.drop_duplicates(subset=[\"intent\"])\n",
        "encoded_df = df.drop(hand_df.index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dL6d610p_kG"
      },
      "source": [
        "# Creating dictionaries for inferencing\n",
        "pn_df = df.drop_duplicates(subset=[\"positive\"])[['positive', 'encode_pn']].sort_values(by=['encode_pn']).set_index('encode_pn')\n",
        "tense_df = df.drop_duplicates(subset=['tense'])[['tense', 'encode_tense']].sort_values(by=['encode_tense']).set_index('encode_tense')\n",
        "form_df = df.drop_duplicates(subset=['form'])[['form', 'encode_form']].sort_values(by=['encode_form']).set_index('encode_form')\n",
        "subject_df = df.drop_duplicates(subset=['subject'])[['subject', 'encode_subject']].sort_values(by=['encode_subject']).set_index('encode_subject')\n",
        "primary_df = df.drop_duplicates(subset=['primary'])[['primary', 'encode_primary']].sort_values(by=['encode_primary']).set_index('encode_primary')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDhFMbg9rzEi"
      },
      "source": [
        "pn_dict = pn_df.to_dict('index')\n",
        "tense_dict = tense_df.to_dict('index')\n",
        "form_dict = form_df.to_dict('index')\n",
        "subject_dict = subject_df.to_dict('index')\n",
        "primary_dict = primary_df.to_dict('index')\n",
        "secondary_dict = secondary_df.to_dict('index')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3Mjx-igBQN5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5b6e4d5-8eb8-4874-d198-60a6ec0f0d6c"
      },
      "source": [
        "# Creating the dataset and dataloader\n",
        "\n",
        "train_size = 0.78\n",
        "train_dataset=encoded_df.sample(frac=train_size,random_state=200)\n",
        "test_dataset=encoded_df.drop(train_dataset.index).reset_index(drop=True)\n",
        "train_dataset = pd.concat([train_dataset, hand_df])\n",
        "train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "\n",
        "print(\"FULL Dataset: {}\".format(df.shape))\n",
        "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
        "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
        "\n",
        "training_set = Triage(train_dataset, tokenizer, MAX_LEN)\n",
        "testing_set = Triage(test_dataset, tokenizer, MAX_LEN)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FULL Dataset: (35784, 17)\n",
            "TRAIN Dataset: (28751, 17)\n",
            "TEST Dataset: (7033, 17)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YJKSqtEBsHi"
      },
      "source": [
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XC54A6x83il7"
      },
      "source": [
        "class DistillBERTClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DistillBERTClass, self).__init__()\n",
        "        self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.pn_class = torch.nn.Linear(768, 3)\n",
        "        self.tense_class = torch.nn.Linear(768, 4)\n",
        "        self.form_class = torch.nn.Linear(768, 4)\n",
        "        self.subject_class = torch.nn.Linear(768, 4)\n",
        "        self.primary_class = torch.nn.Linear(768, len(primary_dict))\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hidden_state = output_1[0]\n",
        "        pooler = hidden_state[:, 0]\n",
        "        pooler = self.pre_classifier(pooler)\n",
        "        pooler = torch.nn.ReLU()(pooler)\n",
        "        pooler = self.dropout(pooler)\n",
        "        pn = self.pn_class(pooler)\n",
        "        tense = self.tense_class(pooler)\n",
        "        form = self.form_class(pooler)\n",
        "        subject = self.subject_class(pooler)\n",
        "        primary = self.primary_class(pooler)\n",
        "\n",
        "        return pn, tense, form, subject, primary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qj9XSdNgOK1F"
      },
      "source": [
        "class ElectraSmallClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ElectraSmallClass, self).__init__()\n",
        "        self.l1 = ElectraModel.from_pretrained(\"google/electra-small-discriminator\")\n",
        "        self.pre_classifier = torch.nn.Linear(256, 256)\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.pn_class = torch.nn.Linear(256, 3)\n",
        "        self.tense_class = torch.nn.Linear(256, 4)\n",
        "        self.form_class = torch.nn.Linear(256, 4)\n",
        "        self.subject_class = torch.nn.Linear(256, 4)\n",
        "        self.primary_class = torch.nn.Linear(256, len(primary_dict))\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hidden_state = output_1[0]\n",
        "        pooler = hidden_state[:, 0]\n",
        "        pooler = self.pre_classifier(pooler)\n",
        "        pooler = torch.nn.ReLU()(pooler)\n",
        "        pooler = self.dropout(pooler)\n",
        "        pn = self.pn_class(pooler)\n",
        "        tense = self.tense_class(pooler)\n",
        "        form = self.form_class(pooler)\n",
        "        subject = self.subject_class(pooler)\n",
        "        primary = self.primary_class(pooler)\n",
        "\n",
        "        return pn, tense, form, subject, primary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xev47SEdBup_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2f49ddd-47c3-4543-a51a-06c7d197a416"
      },
      "source": [
        "model = DistillBERTClass()\n",
        "# model = ElectraSmallClass()\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistillBERTClass(\n",
              "  (l1): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (1): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (2): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (3): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (4): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (5): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (dropout): Dropout(p=0.3, inplace=False)\n",
              "  (pn_class): Linear(in_features=768, out_features=3, bias=True)\n",
              "  (tense_class): Linear(in_features=768, out_features=4, bias=True)\n",
              "  (form_class): Linear(in_features=768, out_features=4, bias=True)\n",
              "  (subject_class): Linear(in_features=768, out_features=4, bias=True)\n",
              "  (primary_class): Linear(in_features=768, out_features=100, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTg2l-JKBxlv"
      },
      "source": [
        "# settin optimizer and loss function\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPfmf2atB0LI"
      },
      "source": [
        "# Calculate accuracy\n",
        "def calcuate_accu(big_idx, targets):\n",
        "    n_correct = (big_idx==targets).sum().item()\n",
        "    return n_correct"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LQ7zonqB2Af"
      },
      "source": [
        "# Defining the training function\n",
        "\n",
        "def train(epoch):\n",
        "    pn_tr_loss = 0\n",
        "    tense_tr_loss = 0\n",
        "    form_tr_loss = 0\n",
        "    subject_tr_loss = 0\n",
        "    primary_tr_loss = 0\n",
        "    secondary_tr_loss = 0\n",
        "    n_pn_correct = 0\n",
        "    n_tense_correct = 0\n",
        "    n_form_correct = 0\n",
        "    n_subject_correct = 0\n",
        "    n_primary_correct = 0\n",
        "    n_secondary_correct = 0\n",
        "\n",
        "    nb_tr_steps = 0\n",
        "    nb_tr_examples = 0\n",
        "    model.train()\n",
        "    for _,data in enumerate(training_loader, 0):\n",
        "        ids = data['ids'].to(device, dtype = torch.long)\n",
        "        mask = data['mask'].to(device, dtype = torch.long)\n",
        "        pn_targets = data['pn_targets'].to(device, dtype = torch.long)\n",
        "        tense_targets = data['tense_targets'].to(device, dtype = torch.long)\n",
        "        form_targets = data['form_targets'].to(device, dtype = torch.long)\n",
        "        subject_targets = data['subject_targets'].to(device, dtype = torch.long)\n",
        "        primary_targets = data['primary_targets'].to(device, dtype = torch.long)\n",
        "\n",
        "        pn_out, tense_out, form_out, subject_out, primary_out = model(ids, mask)\n",
        "        pn_loss = loss_function(pn_out, pn_targets)\n",
        "        tense_loss = loss_function(tense_out, tense_targets)\n",
        "        form_loss = loss_function(form_out, form_targets)\n",
        "        subject_loss = loss_function(subject_out, subject_targets)\n",
        "        primary_loss = loss_function(primary_out, primary_targets)\n",
        "\n",
        "        pn_tr_loss += pn_loss.item()\n",
        "        tense_tr_loss += tense_loss.item()\n",
        "        form_tr_loss += form_loss.item()\n",
        "        subject_tr_loss += subject_loss.item()\n",
        "        primary_tr_loss += primary_loss.item()\n",
        "\n",
        "        pn_val, pn_idx = torch.max(pn_out.data, dim=1)\n",
        "        n_pn_correct += calcuate_accu(pn_idx, pn_targets)\n",
        "\n",
        "        tense_val, tense_idx = torch.max(tense_out.data, dim=1)\n",
        "        n_tense_correct += calcuate_accu(tense_idx, tense_targets)\n",
        "\n",
        "        form_val, form_idx = torch.max(form_out.data, dim=1)\n",
        "        n_form_correct += calcuate_accu(form_idx, form_targets)\n",
        "\n",
        "        subject_val, subject_idx = torch.max(subject_out.data, dim=1)\n",
        "        n_subject_correct += calcuate_accu(subject_idx, subject_targets)\n",
        "\n",
        "        primary_val, primary_idx = torch.max(primary_out.data, dim=1)\n",
        "        n_primary_correct += calcuate_accu(primary_idx, primary_targets)\n",
        "\n",
        "        nb_tr_steps += 1\n",
        "        nb_tr_examples+= pn_targets.size(0)\n",
        "        \n",
        "        if _%1000==0:\n",
        "            pn_loss_step = pn_tr_loss/nb_tr_steps\n",
        "            pn_accu_step = (n_pn_correct*100)/nb_tr_examples\n",
        "            \n",
        "            tense_loss_step = tense_tr_loss/nb_tr_steps\n",
        "            tense_accu_step = (n_tense_correct*100)/nb_tr_examples\n",
        "\n",
        "            form_loss_step = form_tr_loss/nb_tr_steps\n",
        "            form_accu_step = (n_form_correct*100)/nb_tr_examples\n",
        "\n",
        "            subject_loss_step = subject_tr_loss/nb_tr_steps\n",
        "            subject_accu_step = (n_subject_correct*100)/nb_tr_examples\n",
        "\n",
        "            primary_loss_step = primary_tr_loss/nb_tr_steps\n",
        "            primary_accu_step = (n_primary_correct*100)/nb_tr_examples\n",
        "\n",
        "            print(f\"Positive/Negative Loss per 1000 steps: {pn_loss_step}\")\n",
        "            print(f\"Positive/Negative per 1000 steps: {pn_accu_step}\")\n",
        "\n",
        "            print(f\"Tense Loss per 1000 steps: {tense_loss_step}\")\n",
        "            print(f\"Tense Accuracy per 1000 steps: {tense_accu_step}\")\n",
        "\n",
        "            print(f\"Form Loss per 1000 steps: {form_loss_step}\")\n",
        "            print(f\"Form Accuracy per 1000 steps: {form_accu_step}\")\n",
        "\n",
        "            print(f\"Subject Loss per 1000 steps: {subject_loss_step}\")\n",
        "            print(f\"Subject Accuracy per 1000 steps: {subject_accu_step}\")\n",
        "\n",
        "            print(f\"Primary Loss per 1000 steps: {primary_loss_step}\")\n",
        "            print(f\"Primary Accuracy per 1000 steps: {primary_accu_step}\")\n",
        "            print()\n",
        "\n",
        "        \n",
        "        loss = pn_loss + tense_loss + form_loss + subject_loss + primary_loss\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        # # When using GPU\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'PN Accuracy for Epoch {epoch}: {(n_pn_correct*100)/nb_tr_examples}')\n",
        "    pn_epoch_loss = pn_tr_loss/nb_tr_steps\n",
        "    print(f\"Training Loss: {pn_epoch_loss}\")\n",
        "\n",
        "    print(f'Tense Accuracy for Epoch {epoch}: {(n_tense_correct*100)/nb_tr_examples}')\n",
        "    tense_epoch_loss = tense_tr_loss/nb_tr_steps\n",
        "    print(f\"Training Loss: {tense_epoch_loss}\")\n",
        "\n",
        "    print(f'Form Accuracy for Epoch {epoch}: {(n_form_correct*100)/nb_tr_examples}')\n",
        "    form_epoch_loss = form_tr_loss/nb_tr_steps\n",
        "    print(f\"Training Loss: {form_epoch_loss}\")\n",
        "  \n",
        "    print(f'Subject Accuracy for Epoch {epoch}: {(n_subject_correct*100)/nb_tr_examples}')\n",
        "    subject_epoch_loss = subject_tr_loss/nb_tr_steps\n",
        "    print(f\"Training Loss: {subject_epoch_loss}\")\n",
        "\n",
        "    print(f'Primary Accuracy for Epoch {epoch}: {(n_primary_correct*100)/nb_tr_examples}')\n",
        "    primary_epoch_loss = primary_tr_loss/nb_tr_steps\n",
        "    print(f\"Training Loss: {primary_epoch_loss}\")\n",
        "\n",
        "    print()\n",
        "\n",
        "    return "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thW9cBU_B8pq",
        "outputId": "39ed77f7-bcda-464a-c2bf-26d2726ad861"
      },
      "source": [
        "# train model\n",
        "for epoch in range(EPOCHS):\n",
        "    train(epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2190: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Positive/Negative Loss per 1000 steps: 1.0436015129089355\n",
            "Positive/Negative per 1000 steps: 62.5\n",
            "Tense Loss per 1000 steps: 1.4297971725463867\n",
            "Tense Accuracy per 1000 steps: 12.5\n",
            "Form Loss per 1000 steps: 1.4056756496429443\n",
            "Form Accuracy per 1000 steps: 25.0\n",
            "Subject Loss per 1000 steps: 1.3669897317886353\n",
            "Subject Accuracy per 1000 steps: 37.5\n",
            "Primary Loss per 1000 steps: 4.5963335037231445\n",
            "Primary Accuracy per 1000 steps: 0.0\n",
            "\n",
            "Positive/Negative Loss per 1000 steps: 0.24983686149365955\n",
            "Positive/Negative per 1000 steps: 91.30869130869131\n",
            "Tense Loss per 1000 steps: 0.36266138952980925\n",
            "Tense Accuracy per 1000 steps: 87.88711288711289\n",
            "Form Loss per 1000 steps: 0.42057408248032485\n",
            "Form Accuracy per 1000 steps: 86.07017982017982\n",
            "Subject Loss per 1000 steps: 0.5602377463917394\n",
            "Subject Accuracy per 1000 steps: 78.75874125874125\n",
            "Primary Loss per 1000 steps: 2.8467230054882022\n",
            "Primary Accuracy per 1000 steps: 33.21678321678322\n",
            "\n",
            "PN Accuracy for Epoch 0: 92.79329414629056\n",
            "Training Loss: 0.21114747009707047\n",
            "Tense Accuracy for Epoch 0: 89.67340266425515\n",
            "Training Loss: 0.30874898252696725\n",
            "Form Accuracy for Epoch 0: 87.98650481722375\n",
            "Training Loss: 0.36407166236413474\n",
            "Subject Accuracy for Epoch 0: 81.60411811763069\n",
            "Training Loss: 0.49547843537648384\n",
            "Primary Accuracy for Epoch 0: 44.11324823484401\n",
            "Training Loss: 2.326177420520623\n",
            "\n",
            "Positive/Negative Loss per 1000 steps: 0.07451364398002625\n",
            "Positive/Negative per 1000 steps: 100.0\n",
            "Tense Loss per 1000 steps: 0.10673005133867264\n",
            "Tense Accuracy per 1000 steps: 100.0\n",
            "Form Loss per 1000 steps: 0.2500397562980652\n",
            "Form Accuracy per 1000 steps: 93.75\n",
            "Subject Loss per 1000 steps: 0.23670898377895355\n",
            "Subject Accuracy per 1000 steps: 93.75\n",
            "Primary Loss per 1000 steps: 1.366943359375\n",
            "Primary Accuracy per 1000 steps: 75.0\n",
            "\n",
            "Positive/Negative Loss per 1000 steps: 0.11685712968085805\n",
            "Positive/Negative per 1000 steps: 96.3036963036963\n",
            "Tense Loss per 1000 steps: 0.16738772614171485\n",
            "Tense Accuracy per 1000 steps: 94.37437562437563\n",
            "Form Loss per 1000 steps: 0.2110414978183739\n",
            "Form Accuracy per 1000 steps: 93.06943056943057\n",
            "Subject Loss per 1000 steps: 0.27926065414198686\n",
            "Subject Accuracy per 1000 steps: 90.15984015984016\n",
            "Primary Loss per 1000 steps: 1.0761776834756107\n",
            "Primary Accuracy per 1000 steps: 72.23401598401598\n",
            "\n",
            "PN Accuracy for Epoch 1: 96.4453410316163\n",
            "Training Loss: 0.10817177008735622\n",
            "Tense Accuracy for Epoch 1: 94.91495947967027\n",
            "Training Loss: 0.1521195071893467\n",
            "Form Accuracy for Epoch 1: 93.46109700532155\n",
            "Training Loss: 0.1974431315808037\n",
            "Subject Accuracy for Epoch 1: 90.91162046537512\n",
            "Training Loss: 0.2629204478465191\n",
            "Primary Accuracy for Epoch 1: 74.94695836666551\n",
            "Training Loss: 0.9615256401586347\n",
            "\n",
            "Positive/Negative Loss per 1000 steps: 0.06879384815692902\n",
            "Positive/Negative per 1000 steps: 100.0\n",
            "Tense Loss per 1000 steps: 0.1298244744539261\n",
            "Tense Accuracy per 1000 steps: 93.75\n",
            "Form Loss per 1000 steps: 0.05601220577955246\n",
            "Form Accuracy per 1000 steps: 100.0\n",
            "Subject Loss per 1000 steps: 0.1081118956208229\n",
            "Subject Accuracy per 1000 steps: 100.0\n",
            "Primary Loss per 1000 steps: 1.5324513912200928\n",
            "Primary Accuracy per 1000 steps: 68.75\n",
            "\n",
            "Positive/Negative Loss per 1000 steps: 0.06877661866010758\n",
            "Positive/Negative per 1000 steps: 97.67732267732268\n",
            "Tense Loss per 1000 steps: 0.09129133160025082\n",
            "Tense Accuracy per 1000 steps: 97.02172827172828\n",
            "Form Loss per 1000 steps: 0.1274644636514631\n",
            "Form Accuracy per 1000 steps: 95.6481018981019\n",
            "Subject Loss per 1000 steps: 0.17022665803061127\n",
            "Subject Accuracy per 1000 steps: 94.13086913086913\n",
            "Primary Loss per 1000 steps: 0.5383232017154699\n",
            "Primary Accuracy per 1000 steps: 86.33241758241758\n",
            "\n",
            "PN Accuracy for Epoch 2: 97.74268721087962\n",
            "Training Loss: 0.06819355357628788\n",
            "Tense Accuracy for Epoch 2: 97.14444714966436\n",
            "Training Loss: 0.08929171320245305\n",
            "Form Accuracy for Epoch 2: 95.8297102709471\n",
            "Training Loss: 0.12452961976275068\n",
            "Subject Accuracy for Epoch 2: 94.03499008730131\n",
            "Training Loss: 0.1711757698113956\n",
            "Primary Accuracy for Epoch 2: 86.45960140516851\n",
            "Training Loss: 0.5236854535726023\n",
            "\n",
            "Positive/Negative Loss per 1000 steps: 0.12761153280735016\n",
            "Positive/Negative per 1000 steps: 93.75\n",
            "Tense Loss per 1000 steps: 0.13195191323757172\n",
            "Tense Accuracy per 1000 steps: 93.75\n",
            "Form Loss per 1000 steps: 0.07573902606964111\n",
            "Form Accuracy per 1000 steps: 100.0\n",
            "Subject Loss per 1000 steps: 0.39964398741722107\n",
            "Subject Accuracy per 1000 steps: 87.5\n",
            "Primary Loss per 1000 steps: 0.9083354473114014\n",
            "Primary Accuracy per 1000 steps: 81.25\n",
            "\n",
            "Positive/Negative Loss per 1000 steps: 0.04994944807521852\n",
            "Positive/Negative per 1000 steps: 98.27672327672327\n",
            "Tense Loss per 1000 steps: 0.06321921426622377\n",
            "Tense Accuracy per 1000 steps: 98.0457042957043\n",
            "Form Loss per 1000 steps: 0.08486139734629654\n",
            "Form Accuracy per 1000 steps: 97.28396603396604\n",
            "Subject Loss per 1000 steps: 0.12153621881383357\n",
            "Subject Accuracy per 1000 steps: 95.748001998002\n",
            "Primary Loss per 1000 steps: 0.3480783447485555\n",
            "Primary Accuracy per 1000 steps: 90.84040959040959\n",
            "\n",
            "PN Accuracy for Epoch 3: 98.21223609613578\n",
            "Training Loss: 0.05005970219746579\n",
            "Tense Accuracy for Epoch 3: 98.00006956279782\n",
            "Training Loss: 0.06344040431283465\n",
            "Form Accuracy for Epoch 3: 97.13053459010122\n",
            "Training Loss: 0.088801861875009\n",
            "Subject Accuracy for Epoch 3: 95.48189628186846\n",
            "Training Loss: 0.12784447787437353\n",
            "Primary Accuracy for Epoch 3: 90.65075997356614\n",
            "Training Loss: 0.35202055899182755\n",
            "\n",
            "Positive/Negative Loss per 1000 steps: 0.0011345072416588664\n",
            "Positive/Negative per 1000 steps: 100.0\n",
            "Tense Loss per 1000 steps: 0.001487660687416792\n",
            "Tense Accuracy per 1000 steps: 100.0\n",
            "Form Loss per 1000 steps: 0.003911254461854696\n",
            "Form Accuracy per 1000 steps: 100.0\n",
            "Subject Loss per 1000 steps: 0.06080726161599159\n",
            "Subject Accuracy per 1000 steps: 100.0\n",
            "Primary Loss per 1000 steps: 0.09990886598825455\n",
            "Primary Accuracy per 1000 steps: 100.0\n",
            "\n",
            "Positive/Negative Loss per 1000 steps: 0.040082940422291574\n",
            "Positive/Negative per 1000 steps: 98.58266733266733\n",
            "Tense Loss per 1000 steps: 0.0474847303060003\n",
            "Tense Accuracy per 1000 steps: 98.4453046953047\n",
            "Form Loss per 1000 steps: 0.06700872802962926\n",
            "Form Accuracy per 1000 steps: 97.77722277722278\n",
            "Subject Loss per 1000 steps: 0.09934910399837886\n",
            "Subject Accuracy per 1000 steps: 96.52847152847153\n",
            "Primary Loss per 1000 steps: 0.26974981681599125\n",
            "Primary Accuracy per 1000 steps: 92.52622377622377\n",
            "\n",
            "PN Accuracy for Epoch 4: 98.66091614204723\n",
            "Training Loss: 0.039939978361706915\n",
            "Tense Accuracy for Epoch 4: 98.40701193001982\n",
            "Training Loss: 0.04863627749919149\n",
            "Form Accuracy for Epoch 4: 97.70094953219018\n",
            "Training Loss: 0.06924672084202307\n",
            "Subject Accuracy for Epoch 4: 96.29925915620326\n",
            "Training Loss: 0.10420191896162408\n",
            "Primary Accuracy for Epoch 4: 92.55330249382631\n",
            "Training Loss: 0.2713609588071787\n",
            "\n",
            "Positive/Negative Loss per 1000 steps: 0.00831083208322525\n",
            "Positive/Negative per 1000 steps: 100.0\n",
            "Tense Loss per 1000 steps: 0.009047579020261765\n",
            "Tense Accuracy per 1000 steps: 100.0\n",
            "Form Loss per 1000 steps: 0.055159103125333786\n",
            "Form Accuracy per 1000 steps: 93.75\n",
            "Subject Loss per 1000 steps: 0.09407815337181091\n",
            "Subject Accuracy per 1000 steps: 100.0\n",
            "Primary Loss per 1000 steps: 0.07189463824033737\n",
            "Primary Accuracy per 1000 steps: 100.0\n",
            "\n",
            "Positive/Negative Loss per 1000 steps: 0.03409376099877702\n",
            "Positive/Negative per 1000 steps: 98.79495504495505\n",
            "Tense Loss per 1000 steps: 0.039527092279773766\n",
            "Tense Accuracy per 1000 steps: 98.7574925074925\n",
            "Form Loss per 1000 steps: 0.0588883770344453\n",
            "Form Accuracy per 1000 steps: 97.92707292707293\n",
            "Subject Loss per 1000 steps: 0.09241947355483855\n",
            "Subject Accuracy per 1000 steps: 96.7032967032967\n",
            "Primary Loss per 1000 steps: 0.22276876503778445\n",
            "Primary Accuracy per 1000 steps: 93.68756243756243\n",
            "\n",
            "PN Accuracy for Epoch 5: 98.78265103822476\n",
            "Training Loss: 0.03500233490902416\n",
            "Tense Accuracy for Epoch 5: 98.65048172237488\n",
            "Training Loss: 0.042074182843832866\n",
            "Form Accuracy for Epoch 5: 97.801815589023\n",
            "Training Loss: 0.061841467141968864\n",
            "Subject Accuracy for Epoch 5: 96.68533268408055\n",
            "Training Loss: 0.09295581920083061\n",
            "Primary Accuracy for Epoch 5: 93.38457792772425\n",
            "Training Loss: 0.2362499365113514\n",
            "\n",
            "Positive/Negative Loss per 1000 steps: 0.06527553498744965\n",
            "Positive/Negative per 1000 steps: 93.75\n",
            "Tense Loss per 1000 steps: 0.03853606805205345\n",
            "Tense Accuracy per 1000 steps: 100.0\n",
            "Form Loss per 1000 steps: 0.08620847761631012\n",
            "Form Accuracy per 1000 steps: 93.75\n",
            "Subject Loss per 1000 steps: 0.004246532451361418\n",
            "Subject Accuracy per 1000 steps: 100.0\n",
            "Primary Loss per 1000 steps: 0.02565019577741623\n",
            "Primary Accuracy per 1000 steps: 100.0\n",
            "\n",
            "Positive/Negative Loss per 1000 steps: 0.03185624825519526\n",
            "Positive/Negative per 1000 steps: 98.85739260739261\n",
            "Tense Loss per 1000 steps: 0.0366111201980569\n",
            "Tense Accuracy per 1000 steps: 98.91358641358642\n",
            "Form Loss per 1000 steps: 0.05564922047732444\n",
            "Form Accuracy per 1000 steps: 98.11438561438561\n",
            "Subject Loss per 1000 steps: 0.07857276560560977\n",
            "Subject Accuracy per 1000 steps: 97.16533466533467\n",
            "Primary Loss per 1000 steps: 0.1961746633287285\n",
            "Primary Accuracy per 1000 steps: 94.35564435564436\n",
            "\n",
            "PN Accuracy for Epoch 6: 98.86264825571284\n",
            "Training Loss: 0.03182599808884778\n",
            "Tense Accuracy for Epoch 6: 98.85917011582205\n",
            "Training Loss: 0.037111381483510765\n",
            "Form Accuracy for Epoch 6: 98.11484817919377\n",
            "Training Loss: 0.05504127438033739\n",
            "Subject Accuracy for Epoch 6: 97.01227783381448\n",
            "Training Loss: 0.0837797053325035\n",
            "Primary Accuracy for Epoch 6: 94.08020590588154\n",
            "Training Loss: 0.20296403646529235\n",
            "\n",
            "Positive/Negative Loss per 1000 steps: 0.00041804893407970667\n",
            "Positive/Negative per 1000 steps: 100.0\n",
            "Tense Loss per 1000 steps: 0.0014825869584456086\n",
            "Tense Accuracy per 1000 steps: 100.0\n",
            "Form Loss per 1000 steps: 0.009339693933725357\n",
            "Form Accuracy per 1000 steps: 100.0\n",
            "Subject Loss per 1000 steps: 0.011711387895047665\n",
            "Subject Accuracy per 1000 steps: 100.0\n",
            "Primary Loss per 1000 steps: 0.026426227763295174\n",
            "Primary Accuracy per 1000 steps: 100.0\n",
            "\n",
            "Positive/Negative Loss per 1000 steps: 0.02935161221580597\n",
            "Positive/Negative per 1000 steps: 98.98851148851149\n",
            "Tense Loss per 1000 steps: 0.03411566848821518\n",
            "Tense Accuracy per 1000 steps: 98.8886113886114\n",
            "Form Loss per 1000 steps: 0.050696590057209966\n",
            "Form Accuracy per 1000 steps: 98.17057942057941\n",
            "Subject Loss per 1000 steps: 0.07422990989844575\n",
            "Subject Accuracy per 1000 steps: 97.18406593406593\n",
            "Primary Loss per 1000 steps: 0.18327217678014715\n",
            "Primary Accuracy per 1000 steps: 94.4493006993007\n",
            "\n",
            "PN Accuracy for Epoch 7: 98.88351709505757\n",
            "Training Loss: 0.031726653269312656\n",
            "Tense Accuracy for Epoch 7: 98.86264825571284\n",
            "Training Loss: 0.034995897970846186\n",
            "Form Accuracy for Epoch 7: 98.23658307537129\n",
            "Training Loss: 0.049816925812093134\n",
            "Subject Accuracy for Epoch 7: 97.17922854857223\n",
            "Training Loss: 0.07732986572397324\n",
            "Primary Accuracy for Epoch 7: 94.1915063823867\n",
            "Training Loss: 0.1918699280658149\n",
            "\n",
            "Positive/Negative Loss per 1000 steps: 0.0011696808505803347\n",
            "Positive/Negative per 1000 steps: 100.0\n",
            "Tense Loss per 1000 steps: 0.00974709540605545\n",
            "Tense Accuracy per 1000 steps: 100.0\n",
            "Form Loss per 1000 steps: 0.009015817195177078\n",
            "Form Accuracy per 1000 steps: 100.0\n",
            "Subject Loss per 1000 steps: 0.018285904079675674\n",
            "Subject Accuracy per 1000 steps: 100.0\n",
            "Primary Loss per 1000 steps: 0.05813221260905266\n",
            "Primary Accuracy per 1000 steps: 100.0\n",
            "\n",
            "Positive/Negative Loss per 1000 steps: 0.02606554240517037\n",
            "Positive/Negative per 1000 steps: 99.12587412587412\n",
            "Tense Loss per 1000 steps: 0.03137990413115158\n",
            "Tense Accuracy per 1000 steps: 98.97602397602398\n",
            "Form Loss per 1000 steps: 0.044777328611805\n",
            "Form Accuracy per 1000 steps: 98.47652347652348\n",
            "Subject Loss per 1000 steps: 0.07240183851406021\n",
            "Subject Accuracy per 1000 steps: 97.34640359640359\n",
            "Primary Loss per 1000 steps: 0.16548492807497667\n",
            "Primary Accuracy per 1000 steps: 94.81768231768231\n",
            "\n",
            "PN Accuracy for Epoch 8: 99.06438036937845\n",
            "Training Loss: 0.027739717855885686\n",
            "Tense Accuracy for Epoch 8: 98.91829849396542\n",
            "Training Loss: 0.03380563144736094\n",
            "Form Accuracy for Epoch 8: 98.3861430906751\n",
            "Training Loss: 0.04575963204923971\n",
            "Subject Accuracy for Epoch 8: 97.28357274529581\n",
            "Training Loss: 0.07354200739005949\n",
            "Primary Accuracy for Epoch 8: 94.5741017703732\n",
            "Training Loss: 0.17561949141498223\n",
            "\n",
            "Positive/Negative Loss per 1000 steps: 0.009038034826517105\n",
            "Positive/Negative per 1000 steps: 100.0\n",
            "Tense Loss per 1000 steps: 0.0004989007138647139\n",
            "Tense Accuracy per 1000 steps: 100.0\n",
            "Form Loss per 1000 steps: 0.010879778303205967\n",
            "Form Accuracy per 1000 steps: 100.0\n",
            "Subject Loss per 1000 steps: 0.0361248217523098\n",
            "Subject Accuracy per 1000 steps: 100.0\n",
            "Primary Loss per 1000 steps: 0.03388624265789986\n",
            "Primary Accuracy per 1000 steps: 100.0\n",
            "\n",
            "Positive/Negative Loss per 1000 steps: 0.023555941863963523\n",
            "Positive/Negative per 1000 steps: 99.18206793206794\n",
            "Tense Loss per 1000 steps: 0.0291465085695515\n",
            "Tense Accuracy per 1000 steps: 99.07592407592408\n",
            "Form Loss per 1000 steps: 0.04049318915147362\n",
            "Form Accuracy per 1000 steps: 98.52647352647352\n",
            "Subject Loss per 1000 steps: 0.06744115915633021\n",
            "Subject Accuracy per 1000 steps: 97.43381618381619\n",
            "Primary Loss per 1000 steps: 0.15746819708853602\n",
            "Primary Accuracy per 1000 steps: 95.14235764235764\n",
            "\n",
            "PN Accuracy for Epoch 9: 99.05742408959688\n",
            "Training Loss: 0.02636938133053088\n",
            "Tense Accuracy for Epoch 9: 99.0261208305798\n",
            "Training Loss: 0.030244101159673385\n",
            "Form Accuracy for Epoch 9: 98.41049006991061\n",
            "Training Loss: 0.0431963919848385\n",
            "Subject Accuracy for Epoch 9: 97.37052624256548\n",
            "Training Loss: 0.07018355179042504\n",
            "Primary Accuracy for Epoch 9: 94.8801780807624\n",
            "Training Loss: 0.16700386528716601\n",
            "\n",
            "Positive/Negative Loss per 1000 steps: 0.033817898482084274\n",
            "Positive/Negative per 1000 steps: 100.0\n",
            "Tense Loss per 1000 steps: 0.00389459147118032\n",
            "Tense Accuracy per 1000 steps: 100.0\n",
            "Form Loss per 1000 steps: 0.0031388348434120417\n",
            "Form Accuracy per 1000 steps: 100.0\n",
            "Subject Loss per 1000 steps: 0.0689450278878212\n",
            "Subject Accuracy per 1000 steps: 100.0\n",
            "Primary Loss per 1000 steps: 0.09073564410209656\n",
            "Primary Accuracy per 1000 steps: 93.75\n",
            "\n",
            "Positive/Negative Loss per 1000 steps: 0.022126040306142163\n",
            "Positive/Negative per 1000 steps: 99.13211788211788\n",
            "Tense Loss per 1000 steps: 0.026251743940356554\n",
            "Tense Accuracy per 1000 steps: 99.11963036963037\n",
            "Form Loss per 1000 steps: 0.037099767026003184\n",
            "Form Accuracy per 1000 steps: 98.71378621378621\n",
            "Subject Loss per 1000 steps: 0.06290928521339519\n",
            "Subject Accuracy per 1000 steps: 97.48376623376623\n",
            "Primary Loss per 1000 steps: 0.1512221082021342\n",
            "Primary Accuracy per 1000 steps: 95.32967032967034\n",
            "\n",
            "PN Accuracy for Epoch 10: 99.13394316719419\n",
            "Training Loss: 0.02452653952072242\n",
            "Tense Accuracy for Epoch 10: 99.09916176828632\n",
            "Training Loss: 0.02772802942478524\n",
            "Form Accuracy for Epoch 10: 98.61570032346701\n",
            "Training Loss: 0.04000293058364529\n",
            "Subject Accuracy for Epoch 10: 97.55138951688637\n",
            "Training Loss: 0.064472946041507\n",
            "Primary Accuracy for Epoch 10: 95.05060693541094\n",
            "Training Loss: 0.15724121039975839\n",
            "\n",
            "Positive/Negative Loss per 1000 steps: 0.013236956670880318\n",
            "Positive/Negative per 1000 steps: 100.0\n",
            "Tense Loss per 1000 steps: 0.002895411103963852\n",
            "Tense Accuracy per 1000 steps: 100.0\n",
            "Form Loss per 1000 steps: 0.2696380615234375\n",
            "Form Accuracy per 1000 steps: 87.5\n",
            "Subject Loss per 1000 steps: 0.0022498357575386763\n",
            "Subject Accuracy per 1000 steps: 100.0\n",
            "Primary Loss per 1000 steps: 0.019878968596458435\n",
            "Primary Accuracy per 1000 steps: 100.0\n",
            "\n",
            "Positive/Negative Loss per 1000 steps: 0.020418191533509114\n",
            "Positive/Negative per 1000 steps: 99.1570929070929\n",
            "Tense Loss per 1000 steps: 0.022710320931289538\n",
            "Tense Accuracy per 1000 steps: 99.2007992007992\n",
            "Form Loss per 1000 steps: 0.03471555248950346\n",
            "Form Accuracy per 1000 steps: 98.67632367632368\n",
            "Subject Loss per 1000 steps: 0.06525943634912258\n",
            "Subject Accuracy per 1000 steps: 97.49000999001\n",
            "Primary Loss per 1000 steps: 0.149945848474019\n",
            "Primary Accuracy per 1000 steps: 95.2485014985015\n",
            "\n",
            "PN Accuracy for Epoch 11: 99.12350874752183\n",
            "Training Loss: 0.02297742356590138\n",
            "Tense Accuracy for Epoch 11: 99.05046780981532\n",
            "Training Loss: 0.02905744934687654\n",
            "Form Accuracy for Epoch 11: 98.50439984696185\n",
            "Training Loss: 0.040815113148062396\n",
            "Subject Accuracy for Epoch 11: 97.41574206114569\n",
            "Training Loss: 0.06681326378856971\n",
            "Primary Accuracy for Epoch 11: 95.10973531355431\n",
            "Training Loss: 0.1540933667926538\n",
            "\n",
            "Positive/Negative Loss per 1000 steps: 0.07430097460746765\n",
            "Positive/Negative per 1000 steps: 93.75\n",
            "Tense Loss per 1000 steps: 0.09153051674365997\n",
            "Tense Accuracy per 1000 steps: 93.75\n",
            "Form Loss per 1000 steps: 0.017692310735583305\n",
            "Form Accuracy per 1000 steps: 100.0\n",
            "Subject Loss per 1000 steps: 0.033400241285562515\n",
            "Subject Accuracy per 1000 steps: 100.0\n",
            "Primary Loss per 1000 steps: 0.1631452441215515\n",
            "Primary Accuracy per 1000 steps: 93.75\n",
            "\n",
            "Positive/Negative Loss per 1000 steps: 0.02073293950596255\n",
            "Positive/Negative per 1000 steps: 99.18206793206794\n",
            "Tense Loss per 1000 steps: 0.024857142503722455\n",
            "Tense Accuracy per 1000 steps: 99.11963036963037\n",
            "Form Loss per 1000 steps: 0.032612049693391335\n",
            "Form Accuracy per 1000 steps: 98.88236763236763\n",
            "Subject Loss per 1000 steps: 0.05936722857649084\n",
            "Subject Accuracy per 1000 steps: 97.77097902097903\n",
            "Primary Loss per 1000 steps: 0.13186436950542652\n",
            "Primary Accuracy per 1000 steps: 95.81668331668331\n",
            "\n",
            "PN Accuracy for Epoch 12: 99.13394316719419\n",
            "Training Loss: 0.022322502371281718\n",
            "Tense Accuracy for Epoch 12: 99.14785572675733\n",
            "Training Loss: 0.024804007947012586\n",
            "Form Accuracy for Epoch 12: 98.67830684150117\n",
            "Training Loss: 0.03679270248635939\n",
            "Subject Accuracy for Epoch 12: 97.63138673437446\n",
            "Training Loss: 0.06244630103248632\n",
            "Primary Accuracy for Epoch 12: 95.33581440645543\n",
            "Training Loss: 0.14506175955642184\n",
            "\n",
            "Positive/Negative Loss per 1000 steps: 0.012803396210074425\n",
            "Positive/Negative per 1000 steps: 100.0\n",
            "Tense Loss per 1000 steps: 0.0037775631062686443\n",
            "Tense Accuracy per 1000 steps: 100.0\n",
            "Form Loss per 1000 steps: 0.0015865523600950837\n",
            "Form Accuracy per 1000 steps: 100.0\n",
            "Subject Loss per 1000 steps: 0.16367581486701965\n",
            "Subject Accuracy per 1000 steps: 93.75\n",
            "Primary Loss per 1000 steps: 0.08607964962720871\n",
            "Primary Accuracy per 1000 steps: 100.0\n",
            "\n",
            "Positive/Negative Loss per 1000 steps: 0.020246268393419416\n",
            "Positive/Negative per 1000 steps: 99.25074925074925\n",
            "Tense Loss per 1000 steps: 0.023867602507363715\n",
            "Tense Accuracy per 1000 steps: 99.1570929070929\n",
            "Form Loss per 1000 steps: 0.040120572837157406\n",
            "Form Accuracy per 1000 steps: 98.5452047952048\n",
            "Subject Loss per 1000 steps: 0.05565591133229434\n",
            "Subject Accuracy per 1000 steps: 97.9020979020979\n",
            "Primary Loss per 1000 steps: 0.13402168179239304\n",
            "Primary Accuracy per 1000 steps: 95.7042957042957\n",
            "\n",
            "PN Accuracy for Epoch 13: 99.1304650273034\n",
            "Training Loss: 0.022819094306810745\n",
            "Tense Accuracy for Epoch 13: 99.14785572675733\n",
            "Training Loss: 0.025033971575608823\n",
            "Form Accuracy for Epoch 13: 98.57396264477757\n",
            "Training Loss: 0.03945078377991734\n",
            "Subject Accuracy for Epoch 13: 97.74268721087962\n",
            "Training Loss: 0.05867541659730099\n",
            "Primary Accuracy for Epoch 13: 95.33233626656464\n",
            "Training Loss: 0.1446281603215398\n",
            "\n",
            "Positive/Negative Loss per 1000 steps: 0.0009506399510428309\n",
            "Positive/Negative per 1000 steps: 100.0\n",
            "Tense Loss per 1000 steps: 0.01322576217353344\n",
            "Tense Accuracy per 1000 steps: 100.0\n",
            "Form Loss per 1000 steps: 0.021726559847593307\n",
            "Form Accuracy per 1000 steps: 100.0\n",
            "Subject Loss per 1000 steps: 0.004313699435442686\n",
            "Subject Accuracy per 1000 steps: 100.0\n",
            "Primary Loss per 1000 steps: 0.024884317070245743\n",
            "Primary Accuracy per 1000 steps: 100.0\n",
            "\n",
            "Positive/Negative Loss per 1000 steps: 0.02078738238841371\n",
            "Positive/Negative per 1000 steps: 99.2007992007992\n",
            "Tense Loss per 1000 steps: 0.022005917604973956\n",
            "Tense Accuracy per 1000 steps: 99.28196803196803\n",
            "Form Loss per 1000 steps: 0.03807052948415783\n",
            "Form Accuracy per 1000 steps: 98.67007992007991\n",
            "Subject Loss per 1000 steps: 0.0580521109861091\n",
            "Subject Accuracy per 1000 steps: 97.74600399600399\n",
            "Primary Loss per 1000 steps: 0.13859309714632229\n",
            "Primary Accuracy per 1000 steps: 95.52947052947053\n",
            "\n",
            "PN Accuracy for Epoch 14: 99.10959618795869\n",
            "Training Loss: 0.022277799725210167\n",
            "Tense Accuracy for Epoch 14: 99.21046224479149\n",
            "Training Loss: 0.024315189877031892\n",
            "Form Accuracy for Epoch 14: 98.6435254425933\n",
            "Training Loss: 0.03776952263249765\n",
            "Subject Accuracy for Epoch 14: 97.71834023164412\n",
            "Training Loss: 0.05831688868141545\n",
            "Primary Accuracy for Epoch 14: 95.39842092448959\n",
            "Training Loss: 0.1413377479260539\n",
            "\n",
            "Positive/Negative Loss per 1000 steps: 0.039053697139024734\n",
            "Positive/Negative per 1000 steps: 100.0\n",
            "Tense Loss per 1000 steps: 0.00313971727155149\n",
            "Tense Accuracy per 1000 steps: 100.0\n",
            "Form Loss per 1000 steps: 0.08116815239191055\n",
            "Form Accuracy per 1000 steps: 93.75\n",
            "Subject Loss per 1000 steps: 0.1127014011144638\n",
            "Subject Accuracy per 1000 steps: 93.75\n",
            "Primary Loss per 1000 steps: 0.09316722303628922\n",
            "Primary Accuracy per 1000 steps: 93.75\n",
            "\n",
            "Positive/Negative Loss per 1000 steps: 0.016723903392154647\n",
            "Positive/Negative per 1000 steps: 99.3006993006993\n",
            "Tense Loss per 1000 steps: 0.01744046224105493\n",
            "Tense Accuracy per 1000 steps: 99.41933066933066\n",
            "Form Loss per 1000 steps: 0.030248711219789223\n",
            "Form Accuracy per 1000 steps: 98.88236763236763\n",
            "Subject Loss per 1000 steps: 0.05173542464400451\n",
            "Subject Accuracy per 1000 steps: 97.98326673326673\n",
            "Primary Loss per 1000 steps: 0.12647020740917866\n",
            "Primary Accuracy per 1000 steps: 95.79795204795205\n",
            "\n",
            "PN Accuracy for Epoch 15: 99.26263434315328\n",
            "Training Loss: 0.01863319774212735\n",
            "Tense Accuracy for Epoch 15: 99.31132830162429\n",
            "Training Loss: 0.020153597918285124\n",
            "Form Accuracy for Epoch 15: 98.77569475844318\n",
            "Training Loss: 0.03386089731809611\n",
            "Subject Accuracy for Epoch 15: 97.77746860978749\n",
            "Training Loss: 0.05768024388433096\n",
            "Primary Accuracy for Epoch 15: 95.54450279990262\n",
            "Training Loss: 0.1354545556247477\n",
            "\n",
            "Positive/Negative Loss per 1000 steps: 0.009146586060523987\n",
            "Positive/Negative per 1000 steps: 100.0\n",
            "Tense Loss per 1000 steps: 0.00012533024710137397\n",
            "Tense Accuracy per 1000 steps: 100.0\n",
            "Form Loss per 1000 steps: 0.0028455262072384357\n",
            "Form Accuracy per 1000 steps: 100.0\n",
            "Subject Loss per 1000 steps: 0.0006709874724037945\n",
            "Subject Accuracy per 1000 steps: 100.0\n",
            "Primary Loss per 1000 steps: 0.003134135389700532\n",
            "Primary Accuracy per 1000 steps: 100.0\n",
            "\n",
            "Positive/Negative Loss per 1000 steps: 0.019430727399936246\n",
            "Positive/Negative per 1000 steps: 99.23826173826174\n",
            "Tense Loss per 1000 steps: 0.02076507204845631\n",
            "Tense Accuracy per 1000 steps: 99.31318681318682\n",
            "Form Loss per 1000 steps: 0.03169227889004258\n",
            "Form Accuracy per 1000 steps: 98.77622377622377\n",
            "Subject Loss per 1000 steps: 0.05549286507979755\n",
            "Subject Accuracy per 1000 steps: 97.8021978021978\n",
            "Primary Loss per 1000 steps: 0.13001122837403573\n",
            "Primary Accuracy per 1000 steps: 95.748001998002\n",
            "\n",
            "PN Accuracy for Epoch 16: 99.20350596500991\n",
            "Training Loss: 0.020064498194950736\n",
            "Tense Accuracy for Epoch 16: 99.27654690271643\n",
            "Training Loss: 0.022921629000094434\n",
            "Form Accuracy for Epoch 16: 98.67482870161038\n",
            "Training Loss: 0.034488993065095036\n",
            "Subject Accuracy for Epoch 16: 97.78442488956905\n",
            "Training Loss: 0.05569953300275031\n",
            "Primary Accuracy for Epoch 16: 95.56189349935654\n",
            "Training Loss: 0.13465126564113514\n",
            "\n",
            "Positive/Negative Loss per 1000 steps: 0.001131091732531786\n",
            "Positive/Negative per 1000 steps: 100.0\n",
            "Tense Loss per 1000 steps: 0.0003591251152101904\n",
            "Tense Accuracy per 1000 steps: 100.0\n",
            "Form Loss per 1000 steps: 0.0012127016671001911\n",
            "Form Accuracy per 1000 steps: 100.0\n",
            "Subject Loss per 1000 steps: 0.003173626260831952\n",
            "Subject Accuracy per 1000 steps: 100.0\n",
            "Primary Loss per 1000 steps: 0.04374866932630539\n",
            "Primary Accuracy per 1000 steps: 100.0\n",
            "\n",
            "Positive/Negative Loss per 1000 steps: 0.019363840724548897\n",
            "Positive/Negative per 1000 steps: 99.32567432567433\n",
            "Tense Loss per 1000 steps: 0.02158012942584571\n",
            "Tense Accuracy per 1000 steps: 99.23201798201798\n",
            "Form Loss per 1000 steps: 0.033348808665577705\n",
            "Form Accuracy per 1000 steps: 98.75124875124875\n",
            "Subject Loss per 1000 steps: 0.050286215198197835\n",
            "Subject Accuracy per 1000 steps: 97.95204795204795\n",
            "Primary Loss per 1000 steps: 0.125369414344249\n",
            "Primary Accuracy per 1000 steps: 95.56068931068931\n",
            "\n",
            "PN Accuracy for Epoch 17: 99.32176272129665\n",
            "Training Loss: 0.020457763668729253\n",
            "Tense Accuracy for Epoch 17: 99.23828736391778\n",
            "Training Loss: 0.022198059808897014\n",
            "Form Accuracy for Epoch 17: 98.75134777920768\n",
            "Training Loss: 0.033819545515577795\n",
            "Subject Accuracy for Epoch 17: 97.8748565267295\n",
            "Training Loss: 0.054574869015637045\n",
            "Primary Accuracy for Epoch 17: 95.53406838023025\n",
            "Training Loss: 0.13045160268322872\n",
            "\n",
            "Positive/Negative Loss per 1000 steps: 0.000656840275041759\n",
            "Positive/Negative per 1000 steps: 100.0\n",
            "Tense Loss per 1000 steps: 0.008157129399478436\n",
            "Tense Accuracy per 1000 steps: 100.0\n",
            "Form Loss per 1000 steps: 0.1714347004890442\n",
            "Form Accuracy per 1000 steps: 93.75\n",
            "Subject Loss per 1000 steps: 0.23285610973834991\n",
            "Subject Accuracy per 1000 steps: 87.5\n",
            "Primary Loss per 1000 steps: 0.018567221239209175\n",
            "Primary Accuracy per 1000 steps: 100.0\n",
            "\n",
            "Positive/Negative Loss per 1000 steps: 0.019810696391304575\n",
            "Positive/Negative per 1000 steps: 99.17582417582418\n",
            "Tense Loss per 1000 steps: 0.022882253879451874\n",
            "Tense Accuracy per 1000 steps: 99.15084915084915\n",
            "Form Loss per 1000 steps: 0.029652062333847786\n",
            "Form Accuracy per 1000 steps: 98.87612387612387\n",
            "Subject Loss per 1000 steps: 0.05735150414313846\n",
            "Subject Accuracy per 1000 steps: 97.8896103896104\n",
            "Primary Loss per 1000 steps: 0.12306226408984801\n",
            "Primary Accuracy per 1000 steps: 95.86663336663337\n",
            "\n",
            "PN Accuracy for Epoch 18: 99.16176828632048\n",
            "Training Loss: 0.020824070572280893\n",
            "Tense Accuracy for Epoch 18: 99.18959340544677\n",
            "Training Loss: 0.02229285321894197\n",
            "Form Accuracy for Epoch 18: 98.78612917811554\n",
            "Training Loss: 0.03245823807339166\n",
            "Subject Accuracy for Epoch 18: 97.87137838683871\n",
            "Training Loss: 0.05619571243347678\n",
            "Primary Accuracy for Epoch 18: 95.65580327640778\n",
            "Training Loss: 0.1287699566682623\n",
            "\n",
            "Positive/Negative Loss per 1000 steps: 3.047106474696193e-05\n",
            "Positive/Negative per 1000 steps: 100.0\n",
            "Tense Loss per 1000 steps: 0.0002477749658282846\n",
            "Tense Accuracy per 1000 steps: 100.0\n",
            "Form Loss per 1000 steps: 0.0001261633588001132\n",
            "Form Accuracy per 1000 steps: 100.0\n",
            "Subject Loss per 1000 steps: 0.0005600236472673714\n",
            "Subject Accuracy per 1000 steps: 100.0\n",
            "Primary Loss per 1000 steps: 0.006295008584856987\n",
            "Primary Accuracy per 1000 steps: 100.0\n",
            "\n",
            "Positive/Negative Loss per 1000 steps: 0.023480895466563692\n",
            "Positive/Negative per 1000 steps: 99.11963036963037\n",
            "Tense Loss per 1000 steps: 0.022818711006233377\n",
            "Tense Accuracy per 1000 steps: 99.22577422577423\n",
            "Form Loss per 1000 steps: 0.03357045636348684\n",
            "Form Accuracy per 1000 steps: 98.81368631368632\n",
            "Subject Loss per 1000 steps: 0.053672706770433884\n",
            "Subject Accuracy per 1000 steps: 97.87087912087912\n",
            "Primary Loss per 1000 steps: 0.11779602371938268\n",
            "Primary Accuracy per 1000 steps: 95.81668331668331\n",
            "\n",
            "PN Accuracy for Epoch 19: 99.15829014642969\n",
            "Training Loss: 0.022070847314456722\n",
            "Tense Accuracy for Epoch 19: 99.26263434315328\n",
            "Training Loss: 0.021732582188788766\n",
            "Form Accuracy for Epoch 19: 98.74786963931689\n",
            "Training Loss: 0.03403722031367487\n",
            "Subject Accuracy for Epoch 19: 97.82268442836771\n",
            "Training Loss: 0.05428101313582926\n",
            "Primary Accuracy for Epoch 19: 95.6766721157525\n",
            "Training Loss: 0.12393255693821059\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HseqOnrUQ4Ay"
      },
      "source": [
        "# defining validation function\n",
        "\n",
        "def valid(model, testing_loader):\n",
        "    model.eval()\n",
        "    n_pn_correct = 0; n_pn_wrong = 0; pn_total = 0\n",
        "    n_tense_correct = 0; n_tense_wrong = 0; tense_total = 0\n",
        "    n_form_correct = 0; n_form_wrong = 0; form_total = 0\n",
        "    n_subject_correct = 0; n_subject_wrong = 0; subject_total = 0\n",
        "    n_primary_correct = 0; n_primary_wrong = 0; primary_total = 0\n",
        "\n",
        "    nb_tr_steps = 0\n",
        "    nb_tr_examples = 0\n",
        "\n",
        "    pn_tr_loss = 0; tense_tr_loss = 0; form_tr_loss = 0; subject_tr_loss = 0; primary_tr_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(testing_loader, 0):\n",
        "            ids = data['ids'].to(device, dtype = torch.long)\n",
        "            mask = data['mask'].to(device, dtype = torch.long)\n",
        "            pn_targets = data['pn_targets'].to(device, dtype = torch.long)\n",
        "            tense_targets = data['tense_targets'].to(device, dtype = torch.long)\n",
        "            form_targets = data['form_targets'].to(device, dtype = torch.long)\n",
        "            subject_targets = data['subject_targets'].to(device, dtype = torch.long)\n",
        "            primary_targets = data['primary_targets'].to(device, dtype = torch.long)\n",
        "\n",
        "            pn_out, tense_out, form_out, subject_out, primary_out = model(ids, mask)\n",
        "            pn_loss = loss_function(pn_out, pn_targets)\n",
        "            tense_loss = loss_function(tense_out, tense_targets)\n",
        "            form_loss = loss_function(form_out, form_targets)\n",
        "            subject_loss = loss_function(subject_out, subject_targets)\n",
        "            primary_loss = loss_function(primary_out, primary_targets)\n",
        "\n",
        "            pn_tr_loss += pn_loss.item()\n",
        "            tense_tr_loss += tense_loss.item()\n",
        "            form_tr_loss += form_loss.item()\n",
        "            subject_tr_loss += subject_loss.item()\n",
        "            primary_tr_loss += primary_loss.item()\n",
        "\n",
        "            pn_val, pn_idx = torch.max(pn_out.data, dim=1)\n",
        "            n_pn_correct += calcuate_accu(pn_idx, pn_targets)\n",
        "\n",
        "            tense_val, tense_idx = torch.max(tense_out.data, dim=1)\n",
        "            n_tense_correct += calcuate_accu(tense_idx, tense_targets)\n",
        "\n",
        "            form_val, form_idx = torch.max(form_out.data, dim=1)\n",
        "            n_form_correct += calcuate_accu(form_idx, form_targets)\n",
        "\n",
        "            subject_val, subject_idx = torch.max(subject_out.data, dim=1)\n",
        "            n_subject_correct += calcuate_accu(subject_idx, subject_targets)\n",
        "\n",
        "            primary_val, primary_idx = torch.max(primary_out.data, dim=1)\n",
        "            n_primary_correct += calcuate_accu(primary_idx, primary_targets)\n",
        "\n",
        "            nb_tr_steps += 1\n",
        "            nb_tr_examples+= pn_targets.size(0)\n",
        "\n",
        "    pn_epoch_loss = pn_tr_loss/nb_tr_steps\n",
        "    pn_epoch_accu = (n_pn_correct*100)/nb_tr_examples\n",
        "    print(\"Positive and Negative\")\n",
        "    print(f\"Loss: {pn_epoch_loss}\")\n",
        "    print(f\"Accuracy: {pn_epoch_accu}\")\n",
        "\n",
        "    tense_epoch_loss = tense_tr_loss/nb_tr_steps\n",
        "    tense_epoch_accu = (n_tense_correct*100)/nb_tr_examples\n",
        "    print(\"Tense\")\n",
        "    print(f\"Loss: {tense_epoch_loss}\")\n",
        "    print(f\"Accuracy: {tense_epoch_accu}\")\n",
        "    \n",
        "    form_epoch_loss = form_tr_loss/nb_tr_steps\n",
        "    form_epoch_accu = (n_form_correct*100)/nb_tr_examples\n",
        "    print(\"Form\")\n",
        "    print(f\"Loss: {form_epoch_loss}\")\n",
        "    print(f\"Accuracy: {form_epoch_accu}\")\n",
        "\n",
        "    subject_epoch_loss = subject_tr_loss/nb_tr_steps\n",
        "    subject_epoch_accu = (n_subject_correct*100)/nb_tr_examples\n",
        "    print(\"Subject\")\n",
        "    print(f\"Loss: {subject_epoch_loss}\")\n",
        "    print(f\"Accuracy: {subject_epoch_accu}\")\n",
        "\n",
        "    primary_epoch_loss = primary_tr_loss/nb_tr_steps\n",
        "    primary_epoch_accu = (n_primary_correct*100)/nb_tr_examples\n",
        "    print(\"Primary\")\n",
        "    print(f\"Loss: {primary_epoch_loss}\")\n",
        "    print(f\"Accuracy: {primary_epoch_accu}\")\n",
        "\n",
        "    return pn_epoch_accu, tense_epoch_accu, form_epoch_accu, subject_epoch_accu, primary_epoch_accu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db_XGjAURAoK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13442b18-c0af-42a3-ac6c-3921990cdc27"
      },
      "source": [
        "# validating model\n",
        "acc = valid(model, testing_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2190: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Positive and Negative\n",
            "Loss: 0.11639929681237349\n",
            "Accuracy: 97.75344803071236\n",
            "Tense\n",
            "Loss: 0.14718686691489272\n",
            "Accuracy: 97.21313806341533\n",
            "Form\n",
            "Loss: 0.19774747259295758\n",
            "Accuracy: 96.36001706242001\n",
            "Subject\n",
            "Loss: 0.24057713887453824\n",
            "Accuracy: 94.08502772643253\n",
            "Primary\n",
            "Loss: 0.63219550981144\n",
            "Accuracy: 88.36911701976398\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3mOzg5iqokA"
      },
      "source": [
        "drive_file_path = \"/content/drive/MyDrive/Lucy Lucas Classification Model\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfEw-FAztrgQ"
      },
      "source": [
        "# storing train_dataset and test_dataset\n",
        "train_dataset.to_csv(drive_file_path + \"train_dataset.csv\")\n",
        "test_dataset.to_csv(drive_file_path + \"test_dataset.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AQvnTCNvmCM"
      },
      "source": [
        "# storing encoded classification values (just in case they change)\n",
        "pn_df.to_csv(drive_file_path + 'encode_pn.csv')\n",
        "tense_df.to_csv(drive_file_path + 'encode_tense.csv')\n",
        "form_df.to_csv(drive_file_path + 'encode_form.csv')\n",
        "subject_df.to_csv(drive_file_path + 'encode_subject.csv')\n",
        "primary_df.to_csv(drive_file_path + 'encode_primary.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffEYNz1ehq7G"
      },
      "source": [
        "# saving model\n",
        "torch.save({\n",
        "    'epoch': EPOCHS,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict()\n",
        "    }, drive_file_path + 'model_and_optimizer_state_dict.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36wZnKqK49L_"
      },
      "source": [
        "# inference function\n",
        "def predict(text, model=model, tokenizer=tokenizer):\n",
        "  input = tokenizer.encode_plus(text, return_tensors='pt')\n",
        "  cuda_input = input.to('cuda:0')\n",
        "  output = model(cuda_input['input_ids'], cuda_input['attention_mask'])\n",
        "\n",
        "  return returnPrediction(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ey1S9UDrUHx5"
      },
      "source": [
        "# inference helper\n",
        "def returnPrediction(outputTensor):\n",
        "  positive = outputTensor[0].tolist()[0]\n",
        "  tense = outputTensor[1].tolist()[0]\n",
        "  form = outputTensor[2].tolist()[0]\n",
        "  subject = outputTensor[3].tolist()[0]\n",
        "  primary = outputTensor[4].tolist()[0]\n",
        "\n",
        "  pos_value = positive.index(max(positive))\n",
        "  tense_value = tense.index(max(tense))\n",
        "  form_value = form.index(max(form))\n",
        "  subject_value = subject.index(max(subject))\n",
        "  primary_value = primary.index(max(primary))\n",
        "\n",
        "  pn_pred = pn_dict[pos_value]['positive']\n",
        "  tense_pred = tense_dict[tense_value]['tense']\n",
        "  form_pred = form_dict[form_value]['form']\n",
        "  subject_pred = subject_dict[subject_value]['subject']\n",
        "  primary_pred = primary_dict[primary_value]['primary']\n",
        "\n",
        "  return pn_pred, tense_pred, form_pred, subject_pred, primary_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FA2V-olOuEWU",
        "outputId": "4dde13e3-6e17-403c-8bee-486b0fb511fb"
      },
      "source": [
        "predict(\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('positive', 'present', 'declarative', 'You', 'romantic', 'name')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtD6MTFY6qrO"
      },
      "source": [
        "# electra-small predict\n",
        "# predict(\"I am hungry\")\n",
        "# ('positive', 'past', 'declarative', 'I', 'state', 'hungry')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}